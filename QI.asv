function [Q, h] = QI(model, gamma, e_qiter)
%UNTITLED2 Summary of this function goes here
%   Detailed explanation goes here

n1 = model.size(1); % The number of possible states 1
n2 = model.size(2); % The number of possible states 2
m = length(model.U{1}); % The number of possible actions
k = 0;

Qo = zeros(n1, n2, m);
Qchange = ones(n1, n2, m) * 999;

while abs(max(max(Qchange))) > e_qiter
    Qn = zeros(n1, n2, m); % Initialize new Q
    for x1 = 1:n1
        for x2 = 1:n2
            for u = 1:m
                % Get reward and next state
                [xnew, r, final] = gridnav_mdp(model, [x1; x2], u);
                Qn(x1, x2, u) = r + gamma * max(Qo(xnew(1), xnew(2), :));
            end
        end
    end
    %Qn
    
    Qchange = Qn - Qo; % Used to get the biggest change
    Qo = Qn; % The current value will be the old value
    k = k + 1;
end

Q = Qn; % Output Q is assigned here

% Get the optimal policy (hopefully)
h = zeros(n1, n2);
for x1 = 1:n1
    for x2 = 1:n2
        [val, ind] = max(Qn(x1, x2, :));
        h(x1, x2) = ind;
    end
end

end

